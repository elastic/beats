////
This file is generated! See scripts/mage/docs_collector.go
////

[[metricbeat-module-ceph]]
== Ceph module

The Ceph module collects metrics by submitting HTTP GET requests to
the https://docs.ceph.com/docs/jewel/man/8/ceph-rest-api/[ceph-rest-api]. The default metricsets are `cluster_disk`, `cluster_health`, `monitor_health`, `pool_disk`, `osd_tree`.

Metricsets connecting to the Ceph REST API uses by default the service exposed on port 5000.
Metricsets using the Ceph Manager Daemon communicate with the API exposed by default on port 8003 (SSL encryption).

[float]
=== Compatibility

The Ceph module is tested with Ceph Jewel (10.2.10) and Ceph Nautilus (14.2.7).

Metricsets with the `mgr_` prefix are compatible with Ceph releases using the Ceph Manager Daemon.

[float]
=== Dashboard

The Ceph module comes with a predefined dashboard showing Ceph cluster related metrics. For example:

image::./images/ceph-overview-dashboard.png[]



[float]
=== Example configuration

The Ceph module supports the standard configuration options that are described
in <<configuration-metricbeat>>. Here is an example configuration:

[source,yaml]
----
metricbeat.modules:
# Metricsets depending on the Ceph REST API (default port: 5000)
- module: ceph
  metricsets: ["cluster_disk", "cluster_health", "monitor_health", "pool_disk", "osd_tree"]
  period: 10s
  hosts: ["localhost:5000"]
  enabled: true

# Metricsets depending on the Ceph Manager Daemon (default port: 8003)
- module: ceph
  metricsets:
    - mgr_cluster_disk
    - mgr_osd_perf
    - mgr_pool_disk
    - mgr_osd_pool_stats
    - mgr_osd_tree
  period: 1m
  hosts: [ "https://localhost:8003" ]
  #username: "user"
  #password: "secret"
----

This module supports TLS connections when using `ssl` config field, as described in <<configuration-ssl>>.
It also supports the options described in <<module-http-config-options>>.

[float]
=== Metricsets

The following metricsets are available:

* <<metricbeat-metricset-ceph-cluster_disk,cluster_disk>>

* <<metricbeat-metricset-ceph-cluster_health,cluster_health>>

* <<metricbeat-metricset-ceph-cluster_status,cluster_status>>

* <<metricbeat-metricset-ceph-mgr_cluster_disk,mgr_cluster_disk>>

* <<metricbeat-metricset-ceph-mgr_cluster_health,mgr_cluster_health>>

* <<metricbeat-metricset-ceph-mgr_osd_perf,mgr_osd_perf>>

* <<metricbeat-metricset-ceph-mgr_osd_pool_stats,mgr_osd_pool_stats>>

* <<metricbeat-metricset-ceph-mgr_osd_tree,mgr_osd_tree>>

* <<metricbeat-metricset-ceph-mgr_pool_disk,mgr_pool_disk>>

* <<metricbeat-metricset-ceph-monitor_health,monitor_health>>

* <<metricbeat-metricset-ceph-osd_df,osd_df>>

* <<metricbeat-metricset-ceph-osd_tree,osd_tree>>

* <<metricbeat-metricset-ceph-pool_disk,pool_disk>>

include::ceph/cluster_disk.asciidoc[]

include::ceph/cluster_health.asciidoc[]

include::ceph/cluster_status.asciidoc[]

include::ceph/mgr_cluster_disk.asciidoc[]

include::ceph/mgr_cluster_health.asciidoc[]

include::ceph/mgr_osd_perf.asciidoc[]

include::ceph/mgr_osd_pool_stats.asciidoc[]

include::ceph/mgr_osd_tree.asciidoc[]

include::ceph/mgr_pool_disk.asciidoc[]

include::ceph/monitor_health.asciidoc[]

include::ceph/osd_df.asciidoc[]

include::ceph/osd_tree.asciidoc[]

include::ceph/pool_disk.asciidoc[]

