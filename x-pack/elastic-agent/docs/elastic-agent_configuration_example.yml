outputs:
  default:
    type: elasticsearch
    api_key: VuaCfGcBCdbkQm-e5aOx:ui2lp2axTNmsyakw9tvNnw
    hosts: ["localhost:9200"]
    ca_sha256: "7lHLiyp4J8m9kw38SJ7SURJP4bXRZv/BNxyyXkCcE/M="
    # Not supported at first
    queue:
      type: disk

  long_term_storage:
    type: elasticsearch
    api_key: VuaCfGcBCdbkQm-e5aOx:ui2lp2axTNmsyakw9tvNnw
    hosts: ["localhost:9200"]
    ca_sha256: "7lHLiyp4J8m9kw38SJ7SURJP4bXRZv/BNxyyXkCcE/M="
    queue:
      type: disk

  monitoring:
    type: elasticsearch
    api_key: VuaCfGcBCdbkQm-e5aOx:ui2lp2axTNmsyakw9tvNnw
    hosts: ["localhost:9200"]
    ca_sha256: "7lHLiyp4J8m9kw38SJ7SURJP4bXRZv/BNxyyXkCcE/M="

settings.monitoring:
  # enabled turns on monitoring of running processes
  enabled: true
  # enables log monitoring
  logs: true
  # enables metrics monitoring
  metrics: true


# Logging

# There are four options for the log output: file, stderr, syslog, eventlog
# The file output is the default.

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: info

# Enable debug output for selected components. To enable all selectors use ["*"]
# Other available selectors are "beat", "publish", "service"
# Multiple selectors can be chained.
#logging.selectors: [ ]

# Send all logging output to stderr. The default is false.
logging.to_stderr: true

# Send all logging output to syslog. The default is false.
#logging.to_syslog: false

# Send all logging output to Windows Event Logs. The default is false.
#logging.to_eventlog: false

# If enabled, {{.BeatName | title}} periodically logs its internal metrics that have changed
# in the last period. For each metric that changed, the delta from the value at
# the beginning of the period is logged. Also, the total values for
# all non-zero internal metrics are logged on shutdown. The default is true.
#logging.metrics.enabled: true

# The period after which to log the internal metrics. The default is 30s.
#logging.metrics.period: 30s

# Logging to rotating files. Set logging.to_files to false to disable logging to
# files.
#logging.to_files: true
#logging.files:
  # Configure the path where the logs are written. The default is the logs directory
  # under the home path (the binary location).
  #path: /var/log/{{.BeatName}}

  # The name of the files where the logs are written to.
  #name: {{.BeatName}}

  # Configure log file size limit. If limit is reached, log file will be
  # automatically rotated
  #rotateeverybytes: 10485760 # = 10MB

  # Number of rotated log files to keep. Oldest files will be deleted first.
  #keepfiles: 7

  # The permissions mask to apply when rotating log files. The default value is 0600.
  # Must be a valid Unix-style file permissions mask expressed in octal notation.
  #permissions: 0600

  # Enable log file rotation on time intervals in addition to size-based rotation.
  # Intervals must be at least 1s. Values of 1m, 1h, 24h, 7*24h, 30*24h, and 365*24h
  # are boundary-aligned with minutes, hours, days, weeks, months, and years as
  # reported by the local system clock. All other intervals are calculated from the
  # Unix epoch. Defaults to disabled.
  #interval: 0

  # Rotate existing logs on startup rather than appending to the existing
  # file. Defaults to true.
  # rotateonstartup: true

# Set to true to log messages in JSON format.
#logging.json: false

# Set to true, to log messages with minimal required Elastic Common Schema (ECS)
# information. Recommended to use in combination with `logging.json=true`
# Defaults to false.
#logging.ecs: false


inputs:
  - type: logfile
    name: epm/nginx
    version: 1.7.0
    dataset.namespace: prod
    constraints?:
      # Contraints look are not final
      - os.platform: { in: "windows" }
      - agent.version: { ">=": "8.0.0" }
    use_output: long_term_storage
    processors?:
    streams:
      - id?: {id}
        enabled?: true # default to true
        dataset.name: nginx.acccess
        paths: /var/log/nginx/access.log
      - id?: {id}
        enabled?: true # default to true
        dataset.name: nginx.error
        paths: /var/log/nginx/error.log
  - type: nginx/metricspackage?:
    name: epm/nginx
    version: 1.7.0
    dataset.namespace: prod
    constraints?:
      # Contraints look are not final
      - os.platform: { in: "windows" }
      - agent.version: { ">=": "8.0.0" }
    use_output: long_term_storage
    streams:
      - id?: {id}
        enabled?: true # default to true
        dataset.name: nginx.stub_status
        metricset: stub_status

#################################################################################################
# Custom Kafka datasource
inputs:
  - type: kafka
    id: kafka-x1
    title: "Consume data from kafka"
    dataset.namespace: prod
    use_output: long_term_storage
    host: localhost:6566
    streams:
      - dataset.name: foo.dataset
        topic: foo
        processors:
          - extract_bro_specifics


#################################################################################################
# System EPM package
inputs:
  - type: system/metrics
    id?: system
    title: Collect system information and metrics
    package:
      name: epm/system
      version: 1.7.0
    streams:
      - id?: {id}
        enabled?: false # default true
        metricset: cpu
        dataset.name: system.cpu
        metrics: ["percentages", "normalized_percentages"]
        period: 10s
      - metricset: memory
        dataset.name: system.memory
        period: 10s
      - metricset: diskio
        dataset.name: system.diskio
        period: 10s
      - metricset: load
        dataset.name: system.load
        period: 10s
      - metricset: memory
        dataset.name: system.memory
        period: 10s
      - metricset: process
        dataset.name: system.process
        processes: ["firefox*"]
        include_top_n:
          by_cpu: 5      # include top 5 processes by CPU
          by_memory: 5   # include top 5 processes by memory
        period: 10s
      - metricset: process_summary
        dataset.name: system.process_summary
        period: 10s
      - metricset: uptime
        dataset.name: system.uptime
        period: 15m
      - metricset: socket_summary
        dataset.name: system.socket_summary
        period: 10s
      - metricset: filesystem
        dataset.name: system.filesystem
        period: 10s
      - metricset: raid
        dataset.name: system.raid
        period: 10s
      - metricset: socket
        dataset.name: system.socket
        period: 10s
      - metricset: service
        dataset.name: system.service
        period: 10s
      - metricset: fsstat
        dataset.name: system.fsstat
        period: 10s
      - metricset: foo
        dataset.name: system.foo
        period: 10s


#################################################################################################
# Elasticsearch package example
inputs:
    - type: log
      id?: my-endpoint
      title: Collect Elasticsearch information
      package:
        name: epm/elasticsearch
        version: 1.7.0
      streams:
      - id?: {id}
        enabled?: true # default to true
        dataset.name: elasticsearch.audit
        paths: [/var/log/elasticsearch/*_access.log, /var/log/elasticsearch/*_audit.log]
      - id?: {id}
        enabled?: true
        dataset.name: elasticsearch.deprecation
        paths: [/var/log/elasticsearch/*_deprecation.log]
      - id?: {id}
        enabled?: true
        dataset.name: elasticsearch.gc
        paths: [/var/log/elasticsearch/*_gc.log, /var/log/elasticsearch/*_gc.log.[0-9]*]
      - id?: {id}
        enabled?: true
        dataset.name: elasticsearch.server
        paths: [/var/log/elasticsearch/*.log]
      - id?: {id}
        enabled?: true
        dataset.name: elasticsearch.slowlog
        paths: [/var/log/elasticsearch/*_index_search_slowlog.log, /var/log/elasticsearch/*_index_indexing_slowlog.log]
    - type: elasticsearch/metrics
      id?: my-endpoint
      title: Collect Elasticsearch information
      package:
        name: epm/elasticsearch
        version: 1.7.0
      hosts: ["http://localhost:9200"]
      hosts: ["http://localhost:9200"]
      # api_key: xxxx
      # username: elastic
      # password: changeme
      # ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]
      # ssl.ca_sha256: BxZ...
      # ssl.certificate: ...
      # ssl.key: ...
      xpack.enabled: true
      streams:
      - id?: {id}
        metricset: ccr
        dataset.name: elasticseach.ccr
        period: 10s
      - id?: {id}
        metricset: cluster_stats
        dataset.name: elasticseach.cluster_stats
        period: 10s
      - id?: {id}
        metricset: enrich
        dataset.name: elasticseach.enrich
        period: 10s
      - id?: {id}
        metricset: index
        dataset.name: elasticseach.index
        period: 10s
      - id?: {id}
        metricset: index_recovery
        dataset.name: elasticseach.index_recovery
        active_only: true
        period: 10s
      - id?: {id}
        metricset: ml_jobs
        dataset.name: elasticseach.ml_jobs
        period: 10s
      - id?: {id}
        metricset: node_stats
        dataset.name: elasticseach.node_stats
        period: 10s
      - id?: {id}
        metricset: shard
        dataset.name: elasticseach.shard
        period: 10s

#################################################################################################
# AWS module
inputs:
  # Looking at the AWS modules, I believe each fileset need to be in their own
  # buckets?
  - type: s3
    id?: my-aws
    title: Collect AWS
    package:
      name: epm/aws
      version: 1.7.0
    credential_profile_name: fb-aws
    #shared_credential_file: /etc/filebeat/aws_credentials
    streams:
    - id?: {id}
      dataset.name: aws.s3
      queue_url: https://sqs.myregion.amazonaws.com/123456/sqs-queue
    - id?: {id}
      dataset.name: aws.s3access
      queue_url: https://sqs.myregion.amazonaws.com/123456/sqs-queue
    - id?: {id}
      dataset.name: aws.vpcflow
      queue_url: https://sqs.myregion.amazonaws.com/123456/sqs-queue
    - id?: {id}
      dataset.name: aws.cloudtrail
      queue_url: https://sqs.myregion.amazonaws.com/123456/sqs-queue
  - type: aws/metrics
    id?: my-aws
    title: Collect AWS
    package:
      name: epm/aws
      version: 1.7.0
    access_key_id: '${AWS_ACCESS_KEY_ID:""}'
    secret_access_key: '${AWS_SECRET_ACCESS_KEY:""}'
    session_token: '${AWS_SESSION_TOKEN:""}'
    #credential_profile_name: test-mb
    #shared_credential_file: ...
    streams:
    - id?: {id}
      metricset: usage
      dataset.name: aws.usage
      period: 5m
    - id?: {id}
      metricset: cloudwatch
      dataset.name: aws.cloudwatch
      period: 5m
      name: ["CPUUtilization", "DiskWriteOps"]
      tags.resource_type_filter: ec2:instance
      #dimensions:
      #  - name: InstanceId
      #    value: i-0686946e22cf9494a
      statistic: ["Average", "Maximum"]
    - id?: {id}
      metricset: ebs
      dataset.name: aws.ebs
      period: 5m
    - id?: {id}
      metricset: ec2
      dataset.name: aws.ec2
      period: 5m
    - id?: {id}
      metricset: elb
      dataset.name: aws.elb
      period: 5m
    - id?: {id}
      metricset: sns
      dataset.name: aws.sns
      period: 5m
    - id?: {id}
      metricset: sqs
      dataset.name: aws.sqs
      period: 5m
    - id?: {id}
      metricset: rds
      dataset.name: aws.rds
      period: 5m
    - id?: {id}
      metricset: billing
      dataset.name: aws.billing
      period: 12h
    - id?: {id}
      metricset: billing
      dataset.name: aws.billing
      period: 12h
    - id?: {id}
      metricset: s3_daily_storage
      dataset.name: aws.s3_daily_storage
      period: 24h
    - id?: {id}
      metricset: s3_request
      dataset.name: aws.s3_request
      period: 24h


#################################################################################################
# Kubernetes
inputs:
  - type: kubernetes-node/metrics
    id?: my-kubernetes
    title: Collect Kubernetes
    package:
      name: epm/kubernetes
      version: 1.7.0
    hosts: ["localhost:10250"]
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    ssl.certificate_authorities:
      - /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
    # username: "user"
    # password: "secret"
    # If kube_config is not set, KUBECONFIG environment variable will be checked
    # and if not present it will fall back to InCluster
    # kube_config: ~/.kube/config
    streams:
      - id?: {id}
        metricset: node
        dataset.name: kubernetes.node
        period: 10s
      - id?: {id}
        metricset: system
        dataset.name: kubernetes.system
        period: 10s
      - id?: {id}
        metricset: pod
        dataset.name: kubernetes.pod
        period: 10s
      - id?: {id}
        metricset: container
        dataset.name: kubernetes.container
        period: 10s
      - id?: {id}
        metricset: container
        dataset.name: kubernetes.container
        period: 10s
      - id?: {id}
        metricset: event
        dataset.name: kubernetes.event
        period: 10s
  - type: kubernetes-state/metrics
    id?: my-kubernetes
    title: Collect Kubernetes
    package:
      name: epm/kubernetes
      version: 1.7.0
    hosts: ["kube-state-metrics:8080"]
    streams:
      - id?: {id}
        metricset: state_node
        dataset.name: kubernetes.node
        period: 10s
      - id?: {id}
        metricset: state_deployment
        dataset.name: kubernetes.deployment
        period: 10s
      - id?: {id}
        metricset: state_replicaset
        dataset.name: kubernetes.replicaset
        period: 10s
      - id?: {id}
        metricset: state_statefulset
        dataset.name: kubernetes.statefulset
        period: 10s
      - id?: {id}
        metricset: state_pod
        dataset.name: kubernetes.pod
        period: 10s
      - id?: {id}
        metricset: state_container
        dataset.name: kubernetes.container
        period: 10s
      - id?: {id}
        metricset: state_container
        dataset.name: kubernetes.container
        period: 10s
      - id?: {id}
        metricset: state_cronjob
        dataset.name: kubernetes.cronjob
        period: 10s
      - id?: {id}
        metricset: state_resourcequota
        dataset.name: kubernetes.resourcequota
        period: 10s
      - id?: {id}
        metricset: state_service
        dataset.name: kubernetes.service
        period: 10s
      - id?: {id}
        metricset: state_persistentvolume
        dataset.name: kubernetes.persistentvolume
        period: 10s
      - id?: {id}
        metricset: state_persistentvolumeclaim
        dataset.name: kubernetes.persistentvolumeclaim
        period: 10s

#################################################################################################
# Docker
inputs:
  - type: docker/metrics
    id?: my-docker
    title: Collect docker
    package:
      name: epm/docker
      version: 1.7.0
    hosts: ["localhost:10250"]
    #labels.dedot: false

    # To connect to Docker over TLS you must specify a client and CA certificate.
    #ssl:
      #certificate_authority: "/etc/pki/root/ca.pem"
      #certificate:           "/etc/pki/client/cert.pem"
      #key:                   "/etc/pki/client/cert.key"
    streams:
      - id?: {id}
        metricset: container
        dataset.name: docker.container
        period: 10s
      - id?: {id}
        metricset: cpu
        dataset.name: docker.cpu
        period: 10s
      - id?: {id}
        metricset: diskio
        dataset.name: docker.diskio
        period: 10s
      - id?: {id}
        metricset: event
        dataset.name: docker.event
        period: 10s
      - id?: {id}
        metricset: healthcheck
        dataset.name: docker.healthcheck
        period: 10s
      - id?: {id}
        metricset: info
        dataset.name: docker.info
        period: 10s
      - id?: {id}
        metricset: memory
        dataset.name: docker.memory
        period: 10s
      - id?: {id}
        metricset: network
        dataset.name: docker.network
        period: 10s

#################################################################################################
### Suricata
#
inputs:
  - type: log
    id?: suricata-x1
    title: Suricata's data
    dataset.namespace: "abc"
    package:
      name: suricata
      version: x.x.x
    streams:
      -  id?: {id}
        type: "typeX"
        dataset.name: suricata.logs
        path: /var/log/surcata/eve.json

#################################################################################################
### suggestion 1
inputs:
  - type: endpoint # Reserved key word
    id?: myendpoint-x1
    title: Endpoint configuration
    dataset.namespace: "canada"
    package:
      name: endpoint
      version: xxx
    streams:
      - type: malware
        detect: true
        prevent: false
        notify_user: false
        threshold: recommended
        platform: windows

      - type: eventing
        api: true
        clr: false
        dll_and_driver_load: false
        dns: true
        file: false
        platform: windows

      - type: malware
        detect: true
        prevent: false
        notify_user: false
        threshold: recommended
        platform: mac

      - type: eventing
        api: true
        clr: false
        dll_and_driver_load: false
        dns: true
        file: false
        platform: mac

      - type: malware
        detect: true
        prevent: false
        notify_user: false
        threshold: recommended
        platform: linux

      - type: eventing
        api: true
        clr: false
        dll_and_driver_load: false
        dns: true
        file: false
        platform: linux

#################################################################################################
### suggestion 2
inputs:
  - type: endpoint # Reserved key word
    id?: myendpoint-1
    title: Endpoint configuration
    dataset.namespace: "canada"
    package:
      name: epm/endpoint # This establish the link with the package and will allow to link it to endpoint app.
      version: xxx
    windows:
        eventing:
            api: true
            clr: false
            dll_and_driver_load: false
            dns: true
              ...
            file: false
        malware:
            detect: true
            prevent: false
            notify_user: false
            threshold: recommended
    mac:
        eventing:
            file: true
            network: false
            process: false
            ...
        malware:
            detect: true
            prevent: false
            notify_user: false
            threshold: recommended
      linux:
        eventing:
            file: true
            network: false
            process: false
