#------------------------------ NetFlow input --------------------------------
# Experimental: Config options for the Netflow/IPFIX collector over UDP input
#- type: netflow
  #enabled: false

  # Address where the NetFlow Collector will bind
  #host: ":2055"

  # Maximum size of the message received over UDP
  #max_message_size: 10KiB

  # List of enabled protocols.
  # Valid values are 'v1', 'v5', 'v6', 'v7', 'v8', 'v9' and 'ipfix'
  #protocols: [ v5, v9, ipfix ]

  # Expiration timeout
  # This is the time before an idle session or unused template is expired.
  # Only applicable to v9 and ipfix protocols. A value of zero disables expiration.
  #expiration_timeout: 30m

  # Share Templates
  # This option allows v9 and ipfix templates to be shared within a session without
  # reference to the origin of the template.
  #
  # Setting this to true is not recommended as it can result in the wrong template
  # being applied under certain conditions, but it may be required for some systems.
  #share_templates: false

  # Queue size limits the number of netflow packets that are queued awaiting
  # processing.
  #queue_size: 8192

  # Custom field definitions for NetFlow V9 / IPFIX.
  # List of files with YAML fields definition.
  #custom_definitions:
  #- path/to/ipfix.yaml
  #- path/to/netflow.yaml

#---------------------------- Google Cloud Pub/Sub Input -----------------------
# Input for reading messages from a Google Cloud Pub/Sub topic subscription.
- type: gcp-pubsub
  enabled: false

  # Google Cloud project ID. Required.
  project_id: my-gcp-project-id

  # Google Cloud Pub/Sub topic name. Required.
  topic: my-gcp-pubsub-topic-name

  # Google Cloud Pub/Sub topic subscription name. Required.
  subscription.name: my-gcp-pubsub-subscription-name

  # Create subscription if it does not exist.
  #subscription.create: true

  # Number of goroutines to create to read from the subscription.
  #subscription.num_goroutines: 1

  # Maximum number of unprocessed messages to allow at any time.
  #subscription.max_outstanding_messages: 1000

  # Path to a JSON file containing the credentials and key used to subscribe.
  credentials_file: ${path.config}/my-pubsub-subscriber-credentials.json

#------------------------------ AWS S3 input --------------------------------
# Beta: Config options for AWS S3 input
#- type: aws-s3
  #enabled: false

  # AWS Credentials
  # If access_key_id and secret_access_key are configured, then use them to make api calls.
  # If not, aws-s3 input will load default AWS config or load with given profile name.
  #access_key_id: '${AWS_ACCESS_KEY_ID:""}'
  #secret_access_key: '${AWS_SECRET_ACCESS_KEY:""}'
  #session_token: '${AWS_SESSION_TOKEN:"”}'
  #credential_profile_name: test-aws-s3-input

  # SQS queue URL to receive messages from (required).
  #queue_url: "https://sqs.us-east-1.amazonaws.com/1234/test-aws-s3-logs-queue"

  # Maximum number of SQS messages that can be inflight at any time.
  #max_number_of_messages: 5

  # Maximum duration of an AWS API call (excluding S3 GetObject calls).
  #api_timeout: 120s

  # Duration that received SQS messages are hidden from subsequent
  # requests after being retrieved by a ReceiveMessage request.
  #visibility_timeout: 300s

  # List of S3 object metadata keys to include in events.
  #include_s3_metadata: []

  # The max number of times an SQS message should be received (retried) before deleting it.
  #sqs.max_receive_count: 5

  # Maximum duration for which the SQS ReceiveMessage call waits for a message
  # to arrive in the queue before returning.
  #sqs.wait_time: 20s

  # Bucket ARN used for polling AWS S3 buckets
  #bucket_arn: arn:aws:s3:::test-s3-bucket

  # Bucket Name used for polling non-AWS S3 buckets
  #non_aws_bucket_name: test-s3-bucket

  # Configures the AWS S3 API to use path style instead of virtual host style (default)
  #path_style: false

  # Overrides the `cloud.provider` field for non-AWS S3 buckets. See docs for auto recognized providers.
  #provider: minio

  # Configures backing up processed files to another (or the same) bucket
  #backup_to_bucket_arn: 'arn:aws:s3:::mybucket'
  #non_aws_backup_to_bucket_name: 'mybucket'

  # Sets a prefix to prepend to object keys when backing up
  #backup_to_bucket_prefix: 'backup/'

  # Controls deletion of objects after backing them up
  #delete_after_backup: false

#------------------------------ AWS CloudWatch input --------------------------------
# Beta: Config options for AWS CloudWatch input
#- type: aws-cloudwatch
  #enabled: false

  # AWS Credentials
  # If access_key_id and secret_access_key are configured, then use them to make api calls.
  # If not, aws-cloudwatch input will load default AWS config or load with given profile name.
  #access_key_id: '${AWS_ACCESS_KEY_ID:""}'
  #secret_access_key: '${AWS_SECRET_ACCESS_KEY:""}'
  #session_token: '${AWS_SESSION_TOKEN:"”}'
  #credential_profile_name: test-aws-s3-input

  # ARN of the log group to collect logs from
  #log_group_arn: "arn:aws:logs:us-east-1:428152502467:log-group:test:*"

  # Name of the log group to collect logs from.
  # Note: region_name is required when log_group_name is given.
  #log_group_name: test

  # The prefix for a group of log group names.
  # Note: `region_name` is required when `log_group_name_prefix` is given.
  # `log_group_name` and `log_group_name_prefix` cannot be given at the same time.
  #log_group_name_prefix: /aws/

  # Region that the specified log group or log group prefix belongs to.
  #region_name: us-east-1

  # A list of strings of log streams names that Filebeat collect log events from.
  #log_streams:
  # - log_stream_name

  # A string to filter the results to include only log events from log streams
  # that have names starting with this prefix.
  #log_stream_prefix: test

  # `start_position` allows user to specify if this input should read log files
  # from the `beginning` or from the `end`.
  # `beginning`: reads from the beginning of the log group (default).
  # `end`: read only new messages from current time minus `scan_frequency` going forward.
  #start_position: beginning

  # This config parameter sets how often Filebeat checks for new log events from the
  # specified log group. Default `scan_frequency` is 1 minute, which means Filebeat
  # will sleep for 1 minute before querying for new logs again.
  #scan_frequency: 1m

  # The maximum duration of AWS API can take. If it exceeds the timeout, AWS API
  # will be interrupted.
  # The default AWS API timeout for a message is 120 seconds.
  # The minimum is 0 seconds.
  #api_timeout: 120s

  # This is used to sleep between AWS `FilterLogEvents` API calls inside the same
  # collection period.
  #api_sleep: 200ms

  # This is used to shift collection start time and end time back in order to
  # collect logs when there is a delay in CloudWatch.
  #latency: 1m

#------------------------------ ETW input --------------------------------
# Beta: Config options for ETW (Event Trace for Windows) input (Only available for Windows)
#- type: etw
  #enabled: false
  #id: etw-dnsserver

  # Path to an .etl file to read from.
  #file: "C:\Windows\System32\Winevt\Logs\Logfile.etl"

  # GUID of an ETW provider.
  # Run 'logman query providers' to list the available providers.
  #provider.guid: {EB79061A-A566-4698-9119-3ED2807060E7}

  # Name of an ETW provider.
  # Run 'logman query providers' to list the available providers.
  #provider.name: Microsoft-Windows-DNSServer

  # Tag to identify created sessions.
  # If missing, its default value is the provider ID prefixed by 'Elastic-'.
  #session_name: DNSServer-Analytical-Trace

  # Filter collected events with a level value that is less than or equal to this level.
  # Allowed values are critical, error, warning, informational, and verbose.
  #trace_level: verbose

  # 8-byte bitmask that enables the filtering of events from specific provider subcomponents.
  # The provider will write a particular event if the event's keyword bits match any of the bits
  # in this bitmask.
  # Run 'logman query providers "<provider.name>"' to list available keywords.
  #match_any_keyword: 0x8000000000000000

  # 8-byte bitmask that enables the filtering of events from
  # specific provider subcomponents. The provider will write a particular
  # event if the event's keyword bits match all of the bits in this bitmask.
  # Run 'logman query providers "<provider.name>"' to list available keywords.
  #match_all_keyword: 0

  # An existing session to read from.
  # Run 'logman query -ets' to list existing sessions.
  #session: UAL_Usermode_Provider
