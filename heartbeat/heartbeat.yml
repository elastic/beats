################### Heartbeat Configuration Example #########################

# This file is an example configuration file highlighting only some common options.
# The heartbeat.reference.yml file in the same directory contains all the supported options
# with detailed comments. You can use it for reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/heartbeat/index.html

############################# Heartbeat ######################################

# Define a directory to load monitor definitions from. Definitions take the form
# of individual yaml files.
heartbeat.config.monitors:
  # Directory + glob pattern to search for configuration files
  path: ${path.config}/monitors.d/*.yml
  # If enabled, heartbeat will periodically check the config.monitors path for changes
  reload.enabled: false
  # How often to check for changes
  reload.period: 5s

# Configure monitors inline
heartbeat.monitors:
- type: http

  # List or urls to query
  urls: ["http://localhost:9200"]

  # Configure task schedule
  schedule: '@every 10s'

  # Total test connection and data exchange timeout
  #timeout: 16s

# heartbeat-8.0.0-*
# heartbeat-8.0.0-meta

#==================== Elasticsearch template setting ==========================

setup.template.settings:
  index.number_of_shards: 1
  index.codec: best_compression
  #_source.enabled: false

setup.data_frame:
  transforms:
    - pipeline:
        id: states-reformat
        description: reformats the aggregation values into something more ECS friendly
        processors:
        - {"rename": {"field": "state.monitor", "target_field": "monitor"}}
        - {"rename": {"field": "monitor_id", "target_field": "monitor.id"}}
        - {"rename": {"field": "state.summary", "target_field": "summary"}}
        - {"rename": {"field": "state.observer", "target_field": "observer"}}
        - {"rename": {"field": "state.checks", "target_field": "checks"}}
        - {"rename": {"field": "state.@timestamp", "target_field": "@timestamp"}}
        - {"rename": {"field": "state.url", "target_field": "url"}}
        - {"remove": {"field": "state"}}
      mappings:
        dynamic: false
        properties:
          "@timestamp":
            type: date
          observer:
            properties:
              geo:
                properties:
                  name:
                    type: keyword
                  location:
                    type: geo_point
          agent:
            properties:
              id:
                type: keyword
          monitor:
            properties:
              id:
                type: keyword
              status:
                type: keyword
              name:
                type: keyword
              ip:
                type: ip
            url:
              properties:
                domain:
                  type: keyword
                  ignore_above: 1024
                fragment:
                  type: keyword
                  ignore_above: 1024
                full:
                  type: keyword
                  ignore_above: 1024
                original:
                  type: keyword
                  ignore_above: 1024
                password:
                  type: keyword
                  ignore_above: 1024
                path:
                  type: keyword
                  ignore_above: 1024
                port:
                  type: long
                query:
                  type: keyword
                  ignore_above: 1024
                scheme:
                  type: keyword
                  ignore_above: 1024
                username:
                  type: keyword
                  ignore_above: 1024
          checks:
            type: nested
            properties:
              "@timestamp":
                type: date
              monitor:
                type: object
                properties:
                  ip:
                    type: ip
                  status:
                    type: keyword
              agent:
                properties:
                  id:
                    type: keyword
              observer:
                properties:
                  geo:
                    type: object
                    properties:
                      name:
                        type: keyword
                      location:
                        type: geo_point
      pivot:
        group_by:
          monitor_id: {"terms": {"field": "monitor.id"}}
        aggregations:
          state:
            scripted_metric:
              init_script: |
                // Globals are values that should be identical across all docs
                // We can cheat a bit by always overwriting these and make the
                // assumption that there is no variation in these across checks
                state.globals = new HashMap();
                // Here we store stuff broken out by agent.id and monitor.id
                // This should correspond to a unique check.
                state.checksByAgentIdIP = new HashMap();
              map_script: |
                Map curCheck = new HashMap();
                String agentId = doc["agent.id"][0];
                String ip = doc["monitor.ip"][0];
                String agentIdIP = agentId + "-" + ip.toString();
                def ts = doc["@timestamp"][0].toInstant().toEpochMilli();

                def lastCheck = state.checksByAgentIdIP[agentId];
                Instant lastTs = lastCheck != null ? lastCheck["@timestamp"] : null;
                if (lastTs != null && lastTs > ts) {
                  return;
                }

                curCheck.put("@timestamp", ts);

                Map agent = new HashMap();
                agent.id = agentId;
                curCheck.put("agent", agent);

                if (state.globals.url == null) {
                  Map url = new HashMap();
                  Collection fields = ["full", "original", "scheme", "username", "password", "domain", "port", "path", "query", "fragment"];
                  url.full = doc["url.full"];
                  for (field in fields) {
                    String docPath = "url." + field;
                    def val = doc[docPath];
                    if (!val.isEmpty()) {
                      url[field] = val[0];
                    }
                  }
                  state.globals.url = url;
                }

                Map monitor = new HashMap();
                monitor.status = doc["monitor.status"][0];
                monitor.ip = ip;
                def monitorName = doc["monitor.name"][0];
                if (monitorName != "") {
                  monitor.name = monitorName;
                }
                curCheck.monitor = monitor;

                if (curCheck.observer == null) {
                  curCheck.observer = new HashMap();
                }
                if (curCheck.observer.geo == null) {
                  curCheck.observer.geo = new HashMap();
                }
                if (!doc["observer.geo.name"].isEmpty()) {
                  curCheck.observer.geo.name = doc["observer.geo.name"][0];
                }
                if (!doc["observer.geo.location"].isEmpty()) {
                  curCheck.observer.geo.location = doc["observer.geo.location"][0];
                }

                state.checksByAgentIdIP[agentIdIP] = curCheck;
              combine_script: |
                return state;
              reduce_script: |
                // The final document
                Map result = new HashMap();

                Map checks = new HashMap();
                Instant maxTs = Instant.ofEpochMilli(0);
                Collection ips = new HashSet();
                Collection geoNames = new HashSet();
                for (state in states) {
                  result.putAll(state.globals);
                  for (entry in state.checksByAgentIdIP.entrySet()) {
                    def agentIdIP = entry.getKey();
                    def check = entry.getValue();
                    def lastBestCheck = checks.get(agentIdIP);
                    def checkTs = Instant.ofEpochMilli(check.get("@timestamp"));

                    if (maxTs.isBefore(checkTs)) { maxTs = checkTs}

                    if (lastBestCheck == null || lastBestCheck.get("@timestamp") < checkTs) {
                      check["@timestamp"] = check["@timestamp"];
                      checks[agentIdIP] = check
                    }


                    ips.add(check.monitor.ip);
                    if (check.observer != null && check.observer.geo != null && check.observer.geo.name != null) {
                      geoNames.add(check.observer.geo.name);
                    }
                  }
                }

                // We just use the values so we can store these as nested docs
                result.checks = checks.values();
                result.put("@timestamp", maxTs);


                Map summary = new HashMap();
                summary.up = checks.entrySet().stream().filter(c -> c.getValue().monitor.status == "up").count();
                summary.down = checks.size() - summary.up;
                result.summary = summary;

                Map monitor = new HashMap();
                monitor.ip = ips;
                monitor.status = summary.down > 0 ? (summary.up > 0 ? "mixed": "down") : "up";
                result.monitor = monitor;

                Map observer = new HashMap();
                Map geo = new HashMap();
                observer.geo = geo;
                geo.name = geoNames;
                result.observer = observer;

                return result;

#================================ General =====================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their own field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging


#============================== Dashboards =====================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

#============================== Kibana =====================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

#============================= Elastic Cloud ==================================

# These settings simplify using heartbeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

#================================ Outputs =====================================

# Configure what output to use when sending the data collected by the beat.

#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Optional protocol and basic auth credentials.
  #protocol: "https"
  #username: "elastic"
  #password: "changeme"

#----------------------------- Logstash output --------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

#================================ Processors =====================================
processors:
  - add_observer_metadata: 
  # Optional, but recommended geo settings for the location heartbeat is running in 
  #geo: 
    # Token describing this location
    #name: us-east-1a

    # Lat, Lon "
    #location: "37.926868, -78.024902"


#================================ Logging =====================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat",
# "publish", "service".
#logging.selectors: ["*"]

#============================== Xpack Monitoring ===============================
# heartbeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
#monitoring.enabled: false

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch output are accepted here as well.
# Note that the settings should point to your Elasticsearch *monitoring* cluster.
# Any setting that is not set is automatically inherited from the Elasticsearch
# output configuration, so if you have the Elasticsearch output configured such
# that it is pointing to your Elasticsearch monitoring cluster, you can simply
# uncomment the following line.
#monitoring.elasticsearch:

#================================= Migration ==================================

# This allows to enable 6.7 migration aliases
#migration.6_to_7.enabled: true
