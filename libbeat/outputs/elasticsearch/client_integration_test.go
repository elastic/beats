// Licensed to Elasticsearch B.V. under one or more contributor
// license agreements. See the NOTICE file distributed with
// this work for additional information regarding copyright
// ownership. Elasticsearch B.V. licenses this file to you under
// the Apache License, Version 2.0 (the "License"); you may
// not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

// This file was contributed to by generative AI

//go:build integration

package elasticsearch

import (
	"context"
	"fmt"
	"math/rand/v2"
	"testing"
	"time"

	"go.elastic.co/apm/v2/apmtest"
	"go.uber.org/zap"

	"github.com/gofrs/uuid/v5"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/elastic/beats/v7/libbeat/beat"
	"github.com/elastic/beats/v7/libbeat/esleg/eslegtest"
	"github.com/elastic/beats/v7/libbeat/idxmgmt"
	"github.com/elastic/beats/v7/libbeat/outputs"
	"github.com/elastic/beats/v7/libbeat/outputs/outest"
	conf "github.com/elastic/elastic-agent-libs/config"
	"github.com/elastic/elastic-agent-libs/logp"
	"github.com/elastic/elastic-agent-libs/logp/logptest"
	"github.com/elastic/elastic-agent-libs/mapstr"
	"github.com/elastic/elastic-agent-libs/monitoring"
)

func TestClientPublishEvent(t *testing.T) {
	index := "beat-int-pub-single-event"
	cfg := map[string]interface{}{
		"index": index,
	}

	testPublishEvent(t, index, cfg)
}

func TestClientPublishEventKerberosAware(t *testing.T) {
	kerberosURL := "http://localhost:9203"
	err := setupRoleMapping(t, kerberosURL)
	if err != nil {
		t.Fatal(err)
	}

	index := "beat-int-pub-single-event-behind-kerb"
	cfg := map[string]interface{}{
		"hosts": kerberosURL,
		"index": index,
		"kerberos": map[string]interface{}{
			"auth_type":   "password",
			"config_path": "testdata/krb5.conf",
			"username":    "beats",
			"password":    "testing",
			"realm":       "elastic",
		},
	}

	testPublishEvent(t, index, cfg)
}

func testPublishEvent(t *testing.T, index string, cfg map[string]interface{}) {
	registry := monitoring.NewRegistry()
	output, client := connectTestEs(t, cfg, outputs.NewStats(registry, logp.NewNopLogger()))

	// drop old index preparing test
	_, _, _ = client.conn.Delete(index, "", "", nil)

	batch := encodeBatch[*outest.Batch](client, outest.NewBatch(beat.Event{
		Timestamp: time.Now(),
		Fields: mapstr.M{
			"type":    "libbeat",
			"message": "Test message from libbeat",
		},
	}))

	err := output.Publish(context.Background(), batch)
	if err != nil {
		t.Fatal(err)
	}

	_, _, err = client.conn.Refresh(index)
	if err != nil {
		t.Fatal(err)
	}

	_, resp, err := client.conn.CountSearchURI(index, "", nil)
	if err != nil {
		t.Fatal(err)
	}

	assert.Equal(t, 1, resp.Count)

	outputSnapshot := monitoring.CollectFlatSnapshot(registry, monitoring.Full, true)
	assert.Positive(t, outputSnapshot.Ints["write.bytes"], "output.events.write.bytes must be greater than 0")
	assert.Positive(t, outputSnapshot.Ints["read.bytes"], "output.events.read.bytes must be greater than 0")
	assert.Equal(t, int64(0), outputSnapshot.Ints["write.errors"])
	assert.Equal(t, int64(0), outputSnapshot.Ints["read.errors"])
}

func TestClientPublishEventWithPipeline(t *testing.T) {
	type obj map[string]interface{}

	index := "beat-int-pub-single-with-pipeline"
	pipeline := "beat-int-pub-single-pipeline"

	output, client := connectTestEs(t, obj{
		"index":    index,
		"pipeline": "%{[pipeline]}",
	}, nil)
	_, _, _ = client.conn.Delete(index, "", "", nil)

	// Check version
	if client.conn.GetVersion().Major < 5 {
		t.Skip("Skipping tests as pipeline not available in <5.x releases")
	}

	publish := func(event beat.Event) {
		batch := encodeBatch[*outest.Batch](client, outest.NewBatch(event))
		err := output.Publish(context.Background(), batch)
		if err != nil {
			t.Fatal(err)
		}
	}

	getCount := func(query string) int {
		_, resp, err := client.conn.CountSearchURI(index, "", map[string]string{
			"q": query,
		})
		if err != nil {
			t.Fatal(err)
		}
		return resp.Count
	}

	pipelineBody := obj{
		"description": "Test pipeline",
		"processors": []obj{
			{
				"set": obj{
					"field": "testfield",
					"value": 1,
				},
			},
		},
	}

	_, _, _ = client.conn.DeletePipeline(pipeline, nil)
	_, resp, err := client.conn.CreatePipeline(pipeline, nil, pipelineBody)
	if err != nil {
		t.Fatal(err)
	}
	if !resp.Acknowledged {
		t.Fatalf("Test pipeline %v not created", pipeline)
	}

	publish(beat.Event{
		Timestamp: time.Now(),
		Fields: mapstr.M{
			"type":      "libbeat",
			"message":   "Test message 1",
			"pipeline":  pipeline,
			"testfield": 0,
		}})
	publish(beat.Event{
		Timestamp: time.Now(),
		Fields: mapstr.M{
			"type":      "libbeat",
			"message":   "Test message 2",
			"testfield": 0,
		}})

	_, _, err = client.conn.Refresh(index)
	if err != nil {
		t.Fatal(err)
	}

	assert.Equal(t, 1, getCount("testfield:1")) // with pipeline 1
	assert.Equal(t, 1, getCount("testfield:0")) // no pipeline
}

func TestClientBulkPublishEventsWithDeadletterIndex(t *testing.T) {
	type obj map[string]interface{}

	index := "beat-int-test-dli-index"
	deadletterIndex := "beat-int-test-dli-dead-letter-index"

	output, client := connectTestEs(t, obj{
		"index": index,
		"non_indexable_policy": map[string]interface{}{
			"dead_letter_index": map[string]interface{}{
				"index": deadletterIndex,
			},
		},
	}, nil)
	_, _, _ = client.conn.Delete(index, "", "", nil)
	_, _, _ = client.conn.Delete(deadletterIndex, "", "", nil)

	batch := encodeBatch[*outest.Batch](client, outest.NewBatch(beat.Event{
		Timestamp: time.Now(),
		Fields: mapstr.M{
			"type":      "libbeat",
			"message":   "Test message 1",
			"testfield": 0,
		},
	}))
	err := output.Publish(context.Background(), batch)
	if err != nil {
		t.Fatal(err)
	}

	batch = encodeBatch[*outest.Batch](client, outest.NewBatch(beat.Event{
		Timestamp: time.Now(),
		Fields: mapstr.M{
			"type":      "libbeat",
			"message":   "Test message 2",
			"testfield": "foo0",
		},
	}))
	_ = output.Publish(context.Background(), batch)
	_, _, err = client.conn.Refresh(deadletterIndex)
	if err == nil {
		t.Fatal("expecting index to not exist yet")
	}
	err = output.Publish(context.Background(), batch)
	if err != nil {
		t.Fatal(err)
	}

	_, _, err = client.conn.Refresh(index)
	if err != nil {
		t.Fatal(err)
	}

	_, _, err = client.conn.Refresh(deadletterIndex)
	if err != nil {
		t.Fatal(err)
	}

}

func TestClientBulkPublishEventsWithPipeline(t *testing.T) {
	type obj map[string]interface{}

	index := "beat-int-pub-bulk-with-pipeline"
	pipeline := "beat-int-pub-bulk-pipeline"

	output, client := connectTestEs(t, obj{
		"index":    index,
		"pipeline": "%{[pipeline]}",
	}, nil)
	_, _, _ = client.conn.Delete(index, "", "", nil)

	if client.conn.GetVersion().Major < 5 {
		t.Skip("Skipping tests as pipeline not available in <5.x releases")
	}

	publish := func(events ...beat.Event) {
		batch := encodeBatch[*outest.Batch](client, outest.NewBatch(events...))
		err := output.Publish(context.Background(), batch)
		if err != nil {
			t.Fatal(err)
		}
	}

	getCount := func(query string) int {
		_, resp, err := client.conn.CountSearchURI(index, "", map[string]string{
			"q": query,
		})
		if err != nil {
			t.Fatal(err)
		}
		return resp.Count
	}

	pipelineBody := obj{
		"description": "Test pipeline",
		"processors": []obj{
			{
				"set": obj{
					"field": "testfield",
					"value": 1,
				},
			},
		},
	}

	_, _, _ = client.conn.DeletePipeline(pipeline, nil)
	_, resp, err := client.conn.CreatePipeline(pipeline, nil, pipelineBody)
	if err != nil {
		t.Fatal(err)
	}
	if !resp.Acknowledged {
		t.Fatalf("Test pipeline %v not created", pipeline)
	}

	publish(
		beat.Event{
			Timestamp: time.Now(),
			Fields: mapstr.M{
				"type":      "libbeat",
				"message":   "Test message 1",
				"pipeline":  pipeline,
				"testfield": 0,
			}},
		beat.Event{
			Timestamp: time.Now(),
			Fields: mapstr.M{
				"type":      "libbeat",
				"message":   "Test message 2",
				"testfield": 0,
			}},
	)

	_, _, err = client.conn.Refresh(index)
	if err != nil {
		t.Fatal(err)
	}

	assert.Equal(t, 1, getCount("testfield:1")) // with pipeline 1
	assert.Equal(t, 1, getCount("testfield:0")) // no pipeline
}

func TestClientPublishTracer(t *testing.T) {
	index := "beat-apm-tracer-test"
	output, client := connectTestEs(t, map[string]interface{}{
		"index": index,
	}, nil)

	_, _, _ = client.conn.Delete(index, "", "", nil)

	batch := encodeBatch[*outest.Batch](client, outest.NewBatch(beat.Event{
		Timestamp: time.Now(),
		Fields: mapstr.M{
			"message": "Hello world",
		},
	}))

	tx, spans, _ := apmtest.WithTransaction(func(ctx context.Context) {
		err := output.Publish(ctx, batch)
		if err != nil {
			t.Fatal(err)
		}
	})
	require.Len(t, spans, 2)

	// get spans in reverse order
	firstSpan := spans[1]

	assert.Equal(t, "publishEvents", firstSpan.Name)
	assert.Equal(t, "output", firstSpan.Type)
	assert.Equal(t, [8]byte(firstSpan.TransactionID), [8]byte(tx.ID))
	assert.NotEmpty(t, firstSpan.Context.Tags, "no tags found")

	secondSpan := spans[0]
	assert.Contains(t, secondSpan.Name, "POST")
	assert.Equal(t, "db", secondSpan.Type)
	assert.Equal(t, "elasticsearch", secondSpan.Subtype)
	assert.Equal(t, [8]byte(secondSpan.ParentID), [8]byte(firstSpan.ID))
	assert.Equal(t, [8]byte(secondSpan.TransactionID), [8]byte(tx.ID))
	assert.Equal(t, "/_bulk", secondSpan.Context.HTTP.URL.Path)
}

func connectTestEs(t *testing.T, cfg interface{}, stats outputs.Observer) (outputs.Client, *Client) {
	t.Helper()
	if stats == nil {
		stats = outputs.NewNilObserver()
	}
	config, err := conf.NewConfigFrom(map[string]interface{}{
		"hosts":            eslegtest.GetEsHost(),
		"username":         eslegtest.GetUser(),
		"password":         eslegtest.GetPass(),
		"template.enabled": false,
	})
	if err != nil {
		t.Fatal(err)
	}

	tmp, err := conf.NewConfigFrom(cfg)
	if err != nil {
		t.Fatal(err)
	}

	err = config.Merge(tmp)
	if err != nil {
		t.Fatal(err)
	}

	logger := logptest.NewTestingLogger(t, "elasticsearch", zap.AddCallerSkip(1))
	info := beat.Info{Beat: "libbeat", Logger: logger}
	// disable ILM if using specified index name
	im, _ := idxmgmt.DefaultSupport(info, conf.MustNewConfigFrom(map[string]interface{}{"setup.ilm.enabled": "false"}))
	output, err := makeES(im, info, stats, config)
	if err != nil {
		t.Fatal(err)
	}

	type clientWrap interface {
		outputs.NetworkClient
		Client() outputs.NetworkClient
	}
	client, ok := randomClient(output).(clientWrap).Client().(*Client)
	assert.True(t, ok)

	// Load version ctx
	ctx, cancel := context.WithCancel(context.Background())
	t.Cleanup(cancel)
	if err := client.Connect(ctx); err != nil {
		t.Fatalf("cannot connect to ES: %s", err)
	}

	return client, client
}

// setupRoleMapping sets up role mapping for the Kerberos user beats@elastic
func setupRoleMapping(t *testing.T, host string) error {
	_, client := connectTestEs(t, map[string]interface{}{
		"hosts":    host,
		"username": "admin",
		"password": "testing",
	}, nil)

	roleMappingURL := client.conn.URL + "/_security/role_mapping/kerbrolemapping"

	status, _, err := client.conn.RequestURL("POST", roleMappingURL, map[string]interface{}{
		"roles":   []string{"superuser"},
		"enabled": true,
		"rules": map[string]interface{}{
			"field": map[string]interface{}{
				"username": "beats@elastic",
			},
		},
	})

	if status >= 300 {
		return fmt.Errorf("non-2xx return code: %d", status)
	}

	return err
}

func randomClient(grp outputs.Group) outputs.NetworkClient {
	L := len(grp.Clients)
	if L == 0 {
		panic("no elasticsearch client")
	}

	client := grp.Clients[rand.IntN(L)]
	return client.(outputs.NetworkClient) //nolint:errcheck //This is a test file, can ignore
}

// configureDatastreamFailureStore creates an index template with the failure
// store enabled and two mapped fields. The index template will match the passed
// data stream (ds).
func configureDatastreamFailureStore(t *testing.T, client *Client, ds string) {
	templateBody := map[string]any{
		"index_patterns": []string{ds + "*"},
		"data_stream":    map[string]any{},
		"template": map[string]any{
			"data_stream_options": map[string]any{
				"failure_store": map[string]any{
					"enabled": true,
				},
			},
			"mappings": map[string]any{
				"properties": map[string]any{
					"answer": map[string]any{
						"type": "integer",
					},
					"not_a_number": map[string]any{
						"type": "keyword",
					},
				},
			},
		},
	}

	status, resp, err := client.conn.Request("PUT", "/_index_template/idx-tmpl-"+ds, "", nil, templateBody)
	if err != nil {
		t.Fatalf("failed to configure datastream %s: %v", ds, err)
	}
	if status >= 300 {
		t.Fatalf("unexpected status code %d while configuring datastream %s: %s", status, ds, resp)
	}
}

// createDatastreamFailureStore creates and initialises a data stream with the
// failure store enabled used to test the output failure store metrics.
func createDatastreamFailureStore(t *testing.T, client *Client, ds string) {
	configureDatastreamFailureStore(t, client, ds)
	timestamp := time.Now().Format(time.RFC3339)
	body := []any{
		map[string]any{
			"create": map[string]any{},
		},
		map[string]any{
			"@timestamp":   timestamp,
			"answer":       42,
			"not_a_number": "forty two",
		},
	}

	// We need to manually send a bulk request because Publish sets the index
	// in the body, which causes ES to create an index instead of a data stream.
	// An index does not use the failure store.
	status, _, err := client.conn.Bulk(t.Context(), ds, "", nil, nil, body)
	if err != nil {
		t.Fatalf("failed to create datastream %s: %v", ds, err)
	}
	if status >= 300 {
		t.Fatalf("unexpected status code %d while creating datastream %s", status, ds)
	}
}

func TestFailureStoreOutputMetrics(t *testing.T) {
	ds := "test-failure-store-" + uuid.Must(uuid.NewV4()).String()
	registry := monitoring.NewRegistry()

	cfg := map[string]any{
		"index": ds,
	}
	output, client := connectTestEs(t, cfg, outputs.NewStats(registry, logp.NewNopLogger()))

	createDatastreamFailureStore(t, client, ds)

	batch := encodeBatch(client, outest.NewBatch(
		beat.Event{
			Timestamp: time.Now(),
			Fields: mapstr.M{
				"answer":       "forty two",
				"not_a_number": "this should work",
			},
		},
		beat.Event{
			Timestamp: time.Now(),
			Fields: mapstr.M{
				"answer":       42,
				"not_a_number": "forty two",
			},
		},
	))

	err := output.Publish(context.Background(), batch)
	if err != nil {
		t.Fatal(err)
	}

	_, _, err = client.conn.Refresh(ds)
	if err != nil {
		t.Fatal(err)
	}

	_, resp, err := client.conn.CountSearchURI(ds, "", nil)
	if err != nil {
		t.Fatal(err)
	}

	// Expect two events in the index: one from the batch and another
	// from when we created the datastream
	assert.Equal(t, 2, resp.Count)

	outputSnapshot := monitoring.CollectFlatSnapshot(registry, monitoring.Full, true)
	// Ensure the correct number of events was acked and sent to the failure store
	assert.EqualValues(t, 1, outputSnapshot.Ints["events.failure_store"], "failure store metric was not incremented")
	assert.EqualValues(t, 2, outputSnapshot.Ints["events.acked"], "wrong number of acked events")
}
