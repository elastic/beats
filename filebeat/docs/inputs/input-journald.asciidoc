:type: journald

[id="{beatname_lc}-input-{type}"]
=== Journald input

++++
<titleabbrev>journald</titleabbrev>
++++

https://www.freedesktop.org/software/systemd/man/systemd-journald.service.html[`journald`]
is a system service that collects and stores logging data. The `journald` input
reads this log data and the metadata associated with it. To read this
log data {beatname_uc} calls `journalctl` to read from the journal, therefore
{beatname_uc} needs permission to execute `journalctl`.

If the `journalctl` process exits unexpectedly the {type} input will
terminate with an error and {beatname_uc} will need to be
restarted to start reading from the jouranl again.

The simplest configuration example is one that reads all logs from the default
journal.

["source","yaml",subs="attributes"]
----
{beatname_lc}.inputs:
- type: journald
  id: everything
----

You may wish to have separate inputs for each service. You can use
`include_matches` to specify filtering expressions.
A good way to list the https://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html[journald fields] that are available for
filtering messages is to run `journalctl -o json` to output logs and metadata as
JSON. This example collects logs from the `vault.service` systemd unit.

["source","yaml",subs="attributes"]
----
{beatname_lc}.inputs:
- type: journald
  id: service-vault
  include_matches.match:
    - _SYSTEMD_UNIT=vault.service
----

This example collects kernel logs where the message begins with `iptables`.
Note that `include_matches` is more efficient than Beat processors because that
are applied before the data is passed to the {beatname_uc} so prefer them where
possible.

["source","yaml",subs="attributes"]
----
{beatname_lc}.inputs:
- type: journald
  id: iptables
  include_matches.match:
    - _TRANSPORT=kernel
  processors:
    - drop_event:
        when.not.regexp.message: '^iptables'
----

Each example adds the `id` for the input to ensure the cursor is persisted to
the registry with a unique ID. The ID should be unique among journald inputs.
If you don't specify and `id` then one is created for you by hashing
the configuration. So when you modify the config this will result in a new ID
and a fresh cursor.

[id="{beatname_lc}-input-{type}-options"]
==== Configuration options

The `journald` input supports the following configuration options plus the
<<{beatname_lc}-input-{type}-common-options>> described later.

[float]
[id="{beatname_lc}-input-{type}-id"]
==== `id`

An unique identifier for the input. By providing a unique `id` you can
operate multiple inputs on the same journal. This allows each input's cursor to
be persisted independently in the registry file. Each {type} input must have
an unique ID.

["source","yaml",subs="attributes"]
----
{beatname_lc}.inputs:
- type: journald
  id: consul.service
  include_matches.match:
    - _SYSTEMD_UNIT=consul.service

- type: journald
  id: vault.service
  include_matches.match:
    - _SYSTEMD_UNIT=vault.service
----

[float]
[id="{beatname_lc}-input-{type}-paths"]
==== `paths`

A list of paths that will be crawled and fetched. Each path can be a directory
path (to collect events from all journals in a directory), or a file path. If
you specify a directory, {beatname_uc} merges all journals under the directory
into a single journal and reads them.

If no paths are specified, {beatname_uc} reads from the default journal.

[float]
[id="{beatname_lc}-input-{type}-seek"]
==== `seek`

The position to start reading the journal from. Valid settings are:

* `head`: Starts reading at the beginning of the journal. After a restart,
{beatname_uc} resends all log messages in the journal.
* `tail`: Starts reading at the end of the journal. This means that no events
will be sent until a new message is written.
* `since`: Use the `since` option to determine where to start reading from.

Regardless of the value of `seek` if {beatname_uc} has a state (cursor) for this
input, the `seek` value is ignored and the current cursor is used. To reset
the cursor, just change the `id` of the input, this will start from a fresh state.


[float]
[id="{beatname_lc}-input-{type}-since"]
==== `since`

A time offset from the current time to start reading from. To use
`since`, `seek` option must be set to `since`.

This example demonstrates how to resume from the persisted cursor when
it exists, or otherwise begin reading logs from the last 24 hours.

["source","yaml",subs="attributes"]
----
seek: since
since: -24h
----

[float]
[id="{beatname_lc}-input-{type}-units"]
==== `units`

Iterate only the entries of the units specified in this option. The iterated entries include
messages from the units, messages about the units by authorized daemons and coredumps. However,
it does not match systemd user units.

[float]
[id="{beatname_lc}-input-{type}-syslog-identifiers"]
==== `syslog_identifiers`

Read only the entries with the selected syslog identifiers.

[float]
[id="{beatname_lc}-input-{type}-transports"]
==== `transports`

Collect the messages using the specified transports. Example: syslog.

Valid transports:

* audit: messages from the kernel audit subsystem
* driver: internally generated messages
* syslog: messages received via the local syslog socket with the syslog protocol
* journal: messages received via the native journal protocol
* stdout: messages from a service's standard output or error output
* kernel: messages from the kernel

[float]
[id="{beatname_lc}-input-{type}-facilities"]
==== `facilities`

Filter entries by facilities, facilities must be specified using their
numeric code.

[float]
[id="{beatname_lc}-input-{type}-include-matches"]
==== `include_matches`

A collection of filter expressions used to match fields. The format of the expression
is `field=value` or `+` representing disjunction (i.e. logical
OR). {beatname_uc} fetches all events that exactly match the
expressions. Pattern matching is not supported.

When `+` is used, it will cause all matches before and after to be
combined in a disjunction (i.e. logical OR).

If you configured a filter expression, only entries with this field set will be iterated by the journald reader of Filebeat.
If the filter expressions apply to different fields, only entries with all fields set will be iterated.
If they apply to the same fields, only entries where the field takes one of the specified values will be iterated.

`match`: List of filter expressions to match fields.

Please note that these expressions are limited. You can build complex filtering, but full logical
expressions are not supported.

The following include matches configuration will ingest entries that
contain `journald.process.name: systemd` and `systemd.transport: syslog`.

["source","yaml",subs="attributes"]
----
include_matches:
  match:
    - "journald.process.name=systemd"
    - "systemd.transport=syslog"
----

The following include matches configuration will ingest entries that
contain `systemd.transport: systemd` or `systemd.transport: kernel`.

["source","yaml",subs="attributes"]
----
include_matches:
  match:
    - "systemd.transport=kernel"
    - "systemd.transport=syslog"
----


The following include matches configuration is the equivalent of the
following logical expression:

```
A=a OR (B=b AND C=c) OR (D=d AND B=1)
```

["source","yaml",subs="attributes"]
----
include_matches:
  match:
    - A=a
    - +
    - B=b
    - C=c
    - +
    - B=1
----

`include_matches` translates to `journalctl` `MATCHES`, its
[documentation](https://www.man7.org/linux/man-pages/man1/journalctl.1.html)
is not clear about how multiple disjunctions are handled. The previous
example was tested with journalctl version 257.

To reference fields, use one of the following:

* The field name used by the systemd journal. For example,
`CONTAINER_TAG=redis`.
* The <<{beatname_lc}-input-{type}-translated-fields,translated field name>>
used by {beatname_uc}. For example, `container.image.tag=redis`. {beatname_uc}
does not translate all fields from the journal. For custom fields, use the name
specified in the systemd journal.

[float]
===== `parsers`

This option expects a list of parsers that the entry has to go through.

Available parsers:

* `multiline`
* `ndjson`
* `container`
* `syslog`
* `include_message`

In this example, {beatname_uc} is reading multiline messages that consist of 3 lines
and are encapsulated in single-line JSON objects.
The multiline message is stored under the key `msg`.

["source","yaml",subs="attributes"]
----
{beatname_lc}.inputs:
- type: {type}
  ...
  parsers:
    - ndjson:
        target: ""
        message_key: msg
    - multiline:
        type: count
        count_lines: 3
----

See the available parser settings in detail below.

[float]
===== `multiline`

Options that control how {beatname_uc} deals with log messages that span
multiple lines. See <<multiline-examples>> for more information about
configuring multiline options.

[float]
[id="{beatname_lc}-input-{type}-ndjson"]
===== `ndjson`

These options make it possible for {beatname_uc} to decode logs structured as
JSON messages. {beatname_uc} processes the entry by line, so the JSON
decoding only works if there is one JSON object per message.

The decoding happens before line filtering. You can combine JSON
decoding with filtering if you set the `message_key` option. This
can be helpful in situations where the application logs are wrapped in JSON
objects, like when using Docker.

Example configuration:

[source,yaml]
----
- ndjson:
    target: ""
    add_error_key: true
    message_key: log
----

*`target`*:: The name of the new JSON object that should contain the parsed key value pairs. If you
leave it empty, the new keys will go under root.

*`overwrite_keys`*:: Values from the decoded JSON object overwrite the fields that {beatname_uc}
normally adds (type, source, offset, etc.) in case of conflicts. Disable it if you want
to keep previously added values.

*`expand_keys`*:: If this setting is enabled, {beatname_uc} will recursively
de-dot keys in the decoded JSON, and expand them into a hierarchical object
structure. For example, `{"a.b.c": 123}` would be expanded into `{"a":{"b":{"c":123}}}`.
This setting should be enabled when the input is produced by an
https://github.com/elastic/ecs-logging[ECS logger].

*`add_error_key`*:: If this setting is enabled, {beatname_uc} adds an
"error.message" and "error.type: json" key in case of JSON unmarshalling errors
or when a `message_key` is defined in the configuration but cannot be used.

*`message_key`*:: An optional configuration setting that specifies a JSON key on
which to apply the line filtering and multiline settings. If specified the key
must be at the top level in the JSON object and the value associated with the
key must be a string, otherwise no filtering or multiline aggregation will
occur.

*`document_id`*:: Option configuration setting that specifies the JSON key to
set the document id. If configured, the field will be removed from the original
JSON document and stored in `@metadata._id`

*`ignore_decoding_error`*:: An optional configuration setting that specifies if
JSON decoding errors should be logged or not. If set to true, errors will not
be logged. The default is false.

[float]
===== `container`

Use the `container` parser to extract information from  containers log files.
It parses lines into common message lines, extracting timestamps too.

*`stream`*:: Reads from the specified streams only: `all`, `stdout` or `stderr`. The default
is `all`.

*`format`*:: Use the given format when parsing logs: `auto`, `docker` or `cri`. The
default is `auto`, it will automatically detect the format. To disable
autodetection set any of the other options.

The following snippet configures {beatname_uc} to read the `stdout` stream from
all containers under the default Kubernetes logs path:

[source,yaml]
----
  parsers:
    - container:
        stream: stdout
----

[float]
===== `syslog`

The `syslog` parser parses RFC 3146 and/or RFC 5424 formatted syslog messages.

The supported configuration options are:

*`format`*:: (Optional) The syslog format to use, `rfc3164`, or `rfc5424`. To automatically
detect the format from the log entries, set this option to `auto`. The default is `auto`.

*`timezone`*:: (Optional) IANA time zone name(e.g. `America/New York`) or a
fixed time offset (e.g. +0200) to use when parsing syslog timestamps that do not contain
a time zone. `Local` may be specified to use the machine's local time zone. Defaults to `Local`.

*`log_errors`*:: (Optional) If `true` the parser will log syslog parsing errors. Defaults to `false`.

*`add_error_key`*:: (Optional) If this setting is enabled, the parser adds or appends to an
`error.message` key with the parsing error that was encountered. Defaults to `true`.

Example configuration:

[source,yaml]
-------------------------------------------------------------------------------
- syslog:
    format: rfc3164
    timezone: America/Chicago
    log_errors: true
    add_error_key: true
-------------------------------------------------------------------------------

*Timestamps*

The RFC 3164 format accepts the following forms of timestamps:

* Local timestamp (`Mmm dd hh:mm:ss`):
** `Jan 23 14:09:01`
* RFC-3339*:
** `2003-10-11T22:14:15Z`
** `2003-10-11T22:14:15.123456Z`
** `2003-10-11T22:14:15-06:00`
** `2003-10-11T22:14:15.123456-06:00`

*Note*: The local timestamp (for example, `Jan 23 14:09:01`) that accompanies an
RFC 3164 message lacks year and time zone information. The time zone will be enriched
using the `timezone` configuration option, and the year will be enriched using the
{beatname_uc} system's local time (accounting for time zones). Because of this, it is possible
for messages to appear in the future. An example of when this might happen is logs
generated on December 31 2021 are ingested on January 1 2022. The logs would be enriched
with the year 2022 instead of 2021.

The RFC 5424 format accepts the following forms of timestamps:

* RFC-3339:
** `2003-10-11T22:14:15Z`
** `2003-10-11T22:14:15.123456Z`
** `2003-10-11T22:14:15-06:00`
** `2003-10-11T22:14:15.123456-06:00`

Formats with an asterisk (*) are a non-standard allowance.

[float]
===== `include_message`

Use the `include_message` parser to filter messages in the parsers pipeline. Messages that
match the provided pattern are passed to the next parser, the others are dropped.

You should use `include_message` instead of `include_lines` if you would like to
control when the filtering happens. `include_lines` runs after the parsers, `include_message`
runs in the parsers pipeline.

*`patterns`*:: List of regexp patterns to match.

This example shows you how to include messages that start with the string ERR or WARN:

[source,yaml]
----
  parsers:
    - include_message.patterns: ["^ERR", "^WARN"]
----

[float]
[id="{beatname_lc}-input-{type}-translated-fields"]
=== Translated field names

You can use the following translated names in filter expressions to reference
journald fields:

[horizontal]
*Journald field name*:: *Translated name*
`COREDUMP_UNIT`::             `journald.coredump.unit`
`COREDUMP_USER_UNIT`::        `journald.coredump.user_unit`
`OBJECT_AUDIT_LOGINUID`::     `journald.object.audit.login_uid`
`OBJECT_AUDIT_SESSION`::      `journald.object.audit.session`
`OBJECT_CMDLINE`::            `journald.object.cmd`
`OBJECT_COMM`::               `journald.object.name`
`OBJECT_EXE`::                `journald.object.executable`
`OBJECT_GID`::                `journald.object.gid`
`OBJECT_PID`::                `journald.object.pid`
`OBJECT_SYSTEMD_OWNER_UID`::  `journald.object.systemd.owner_uid`
`OBJECT_SYSTEMD_SESSION`::    `journald.object.systemd.session`
`OBJECT_SYSTEMD_UNIT`::       `journald.object.systemd.unit`
`OBJECT_SYSTEMD_USER_UNIT`::  `journald.object.systemd.user_unit`
`OBJECT_UID`::                `journald.object.uid`
`_AUDIT_LOGINUID`::           `process.audit.login_uid`
`_AUDIT_SESSION`::            `process.audit.session`
`_BOOT_ID`::                  `host.boot_id`
`_CAP_EFFECTIVE`::            `process.capabilites`
`_CMDLINE`::                  `process.cmd`
`_CODE_FILE`::                `journald.code.file`
`_CODE_FUNC`::                `journald.code.func`
`_CODE_LINE`::                `journald.code.line`
`_COMM`::                     `process.name`
`_EXE`::                      `process.executable`
`_GID`::                      `process.uid`
`_HOSTNAME`::                 `host.name`
`_KERNEL_DEVICE`::            `journald.kernel.device`
`_KERNEL_SUBSYSTEM`::         `journald.kernel.subsystem`
`_MACHINE_ID`::               `host.id`
`_MESSAGE`::                  `message`
`_PID`::                      `process.pid`
`_PRIORITY`::                 `log.syslog.priority`
`_SYSLOG_FACILITY`::          `log.syslog.facility.code`
`_SYSLOG_IDENTIFIER`::        `log.syslog.appname`
`_SYSLOG_PID`::               `log.syslog.procid`
`_SYSTEMD_CGROUP`::           `systemd.cgroup`
`_SYSTEMD_INVOCATION_ID`::    `systemd.invocation_id`
`_SYSTEMD_OWNER_UID`::        `systemd.owner_uid`
`_SYSTEMD_SESSION`::          `systemd.session`
`_SYSTEMD_SLICE`::            `systemd.slice`
`_SYSTEMD_UNIT`::             `systemd.unit`
`_SYSTEMD_USER_SLICE`::       `systemd.user_slice`
`_SYSTEMD_USER_UNIT`::        `systemd.user_unit`
`_TRANSPORT`::                `systemd.transport`
`_UDEV_DEVLINK`::             `journald.kernel.device_symlinks`
`_UDEV_DEVNODE`::             `journald.kernel.device_node_path`
`_UDEV_SYSNAME`::             `journald.kernel.device_name`
`_UID`::                      `process.uid`

The following translated fields for
https://docs.docker.com/config/containers/logging/journald/[Docker] are also
available:

[horizontal]
`CONTAINER_ID_FULL`::         `container.id`
`CONTAINER_NAME`::            `container.name`
`IMAGE_NAME`::		      `container.image.name`

If `CONTAINER_PARTIAL_MESSAGE` is present and it is true, then the tag
`partial_message` is added to the final event.

[id="{beatname_lc}-input-{type}-common-options"]
include::../inputs/input-common-options.asciidoc[]

:type!:
